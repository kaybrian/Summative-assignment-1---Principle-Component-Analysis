{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaybrian/Summative-assignment-1---Principle-Component-Analysis/blob/main/Summative_Assignment_PCA_Kayongo_Johnson_Brian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary package\n",
        "#TO DO\n",
        "import numpy as np\n",
        "from numpy.linalg import eig\n",
        "\n"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "The aim of standardizing the range of the continuous initial variables is so that each one of them contributes equally to the analysis.\n",
        "\n",
        "More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (for example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.\n"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(data, axis=0, keepdims=True)\n",
        "\n",
        "standardized_data = (data - mean) / np.std(data, axis=0)\n",
        "\n",
        "# test to see if the data is standardized\n",
        "print(standardized_data)\n"
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d43050-0aaa-4f8a-f8b0-2f041383cf0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n, d = data.shape\n",
        "mean = np.mean(data, axis=0, keepdims=True)\n",
        "centered_data = data - mean\n",
        "cov_matrix = np.dot(centered_data.T, centered_data) / (n - 1)\n",
        "\n",
        "\n",
        "print(cov_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F20GMgep3r-2",
        "outputId": "001bf781-e569-4473-e125-ab2f23282b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2.16666667  -1.06666667  -1.76666667   5.5         -4.36666667]\n",
            " [ -1.06666667   4.26666667   0.46666667  -6.6         19.66666667]\n",
            " [ -1.76666667   0.46666667   1.76666667  -3.3          2.36666667]\n",
            " [  5.5         -6.6         -3.3         24.7        -27.9       ]\n",
            " [ -4.36666667  19.66666667   2.36666667 -27.9         92.56666667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "izXfKNO728n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## [[  1.80555556  -0.88888889  -1.47222222   4.58333333  -3.63888889]\n",
        "## [ -0.88888889   3.55555556   0.38888889  -5.5         16.38888889]\n",
        "## [ -1.47222222   0.38888889   1.47222222  -2.75         1.97222222]\n",
        "## [  4.58333333  -5.5         -2.75        20.58333333 -23.25      ]\n",
        "## [ -3.63888889  16.38888889   1.97222222 -23.25        77.13888889]]"
      ],
      "metadata": {
        "id": "9hbigvAk24hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i am using the built in function from Numpy array to get the\n",
        "# eigenvalues, and eigenvectors of cov_matrix\n",
        "\n",
        "# Note: These come from the Covariance Matrix and not the data itself.\n",
        "\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# print the values down to test if they are the right values\n",
        "print('eigenvalues: ', eigenvalues)\n",
        "print('eigenvectors: ', eigenvectors)"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56662f35-ab72-4750-d151-77ffad8a6433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvalues:  [1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n",
            "eigenvectors:  [[-0.05817655 -0.2631212   0.57237125  0.6292347  -0.45148374]\n",
            " [ 0.19774895 -0.03283879  0.06849106 -0.60720902 -0.7657827 ]\n",
            " [ 0.0328828   0.17887983 -0.75671562  0.45776292 -0.42983171]\n",
            " [-0.33200499 -0.88598416 -0.30234056 -0.11461168  0.01609676]\n",
            " [ 0.91989252 -0.33574235 -0.06059523  0.11259736  0.15724145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:,order_of_importance] # sort the columns\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "67b5969f-e189-4cff-f285-18139fdbecb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [0 1 2 3 4]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.05817655 -0.2631212   0.57237125  0.6292347  -0.45148374]\n",
            " [ 0.19774895 -0.03283879  0.06849106 -0.60720902 -0.7657827 ]\n",
            " [ 0.0328828   0.17887983 -0.75671562  0.45776292 -0.42983171]\n",
            " [-0.33200499 -0.88598416 -0.30234056 -0.11461168  0.01609676]\n",
            " [ 0.91989252 -0.33574235 -0.06059523  0.11259736  0.15724145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "By ranking our eigenvectors in order of their eigenvalues, highest to lowest, we get the principal components in order of significance.\n",
        "\n",
        "\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Yes,This will allow us to choose the principal components that explain the most variance in the data."
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "\n",
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T16CBmNxGp-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "sum_of_eigenvalues = np.sum(sorted_eigenvalues)\n",
        "\n",
        "# print the sum of the Eigen values\n",
        "print(sum_of_eigenvalues)\n",
        "\n",
        "# now calculate the percentage value of each eigen value\n",
        "percentages = (sorted_eigenvalues / sum_of_eigenvalues) * 100\n",
        "\n",
        "# print out the percentages of each eigen values\n",
        "\n",
        "print(percentages)\n",
        "\n",
        "# print out the percentages\n",
        "for idx, value in enumerate(sorted_eigenvalues):\n",
        "    print(f\"Eigenvalue {idx+1}: {value:.8f} ({percentages[idx]:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB0rNlgpFlXL",
        "outputId": "90c0c08f-b1d4-47be-dfce-e1ce9194bc3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125.46666666666665\n",
            "[8.54607471e+01 1.28977515e+01 1.53964188e+00 1.01684172e-01\n",
            " 1.75348375e-04]\n",
            "Eigenvalue 1: 107.22475075 (85.46%)\n",
            "Eigenvalue 2: 16.18237882 (12.90%)\n",
            "Eigenvalue 3: 1.93173735 (1.54%)\n",
            "Eigenvalue 4: 0.12757974 (0.10%)\n",
            "Eigenvalue 5: 0.00022000 (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "#TO DO: Insert code here\n",
        "explained_variance = (sorted_eigenvalues / sum_of_eigenvalues) * 100\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print(explained_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "4ad03d53-e81b-43b4-d428-f8ebabc74df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['85.46%', '12.90%', '1.54%', '0.10%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2 # select the number of principal components\n",
        "\n",
        "reduced_data_vectors = sorted_eigenvectors[:, order_of_importance[:k]]\n",
        "reduced_data = np.matmul(standardized_data,reduced_data_vectors)#TO DO: insert code here)[#TO DO: insert code here] # transform the original data"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the value of the reduced data\n",
        "print(reduced_data)\n",
        "\n",
        "# print the head of the updated data here\n",
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUAj8vK2WsFz",
        "outputId": "f7c7dc72-e754-4c16-a537-135a7dfde6e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.31389845  1.22362226]\n",
            " [-2.55511419  0.01760889]\n",
            " [ 0.61494463  1.08892909]\n",
            " [-0.03531847 -1.11250845]\n",
            " [ 1.45756867  0.44379893]\n",
            " [-0.7959791  -1.66145072]]\n",
            "(6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "76eb09ad-ace9-4d15-bb2b-59b67d8092fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.07127878 -0.6983307 ]\n",
            " [-3.49014682 -0.59870297]\n",
            " [ 0.24003422 -0.48244534]\n",
            " [-0.14516166 -1.61189378]\n",
            " [ 1.34022572 -1.07434063]\n",
            " [-0.96573453 -2.23341502]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "78e448df-45c8-45d7-dbfb-6bf810f2a486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "## Benefits\n",
        "\n",
        "- First and formost, the PCA concpet is a dimensionality reduction and machine learning method used to simplify a large data set into a smaller set while still maintaining significant patterns and trends. THis in turn helps to reduce the computation power needed to process and work on the data presented.\n",
        "- Using PCA, you are certain to reduce the complexity and noise of the data, and highlight the most important features and relationships in a given data set.\n",
        "- Its important to note that PCA is a linear and unsupervised method which means that hat it does not require any prior assumptions or labels about the data. This in turn creates an open area for data Exploration and it can be interpreted using standard tools and libraries.\n",
        "\n",
        "\n",
        "## Negative effects\n",
        "\n",
        "- Because some data is lost during the PCA process, its safe to say that this can lead to oversimplification or distortion of the data, and make it harder to identify outliers or anomalies.\n",
        "- Due to the fact that PCA is very sensitive to scaling and outliers, this in turn affects the stablitiy of the results we get after working on the Data."
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    }
  ]
}